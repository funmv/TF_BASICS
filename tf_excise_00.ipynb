{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Mnist 학습 예제로 Tensorflow 연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mnist 데이터셋은 tf 내부에 테스트 샘플로 미리 준비되어 있어 가져와 사용하면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist      = input_data.read_data_sets('data', one_hot=True)    #label은 one-hot벡터로 취함\n",
    "X_train   = mnist.train.images\n",
    "Y_train = mnist.train.labels\n",
    "X_test    = mnist.test.images\n",
    "Y_test  = mnist.test.labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 입력된 mnist 데이터의 tensor의 크기를 출력해 본다. \n",
    "## 데이터는 학습용과 테스트용 2가지 종류이다.\n",
    "### 학습용은 28x28=784 크기의 흑백 영상이 55,000개이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of (X_train, X_test, Y_train, Y_test)\n",
      "((55000, 784), (10000, 784), (55000, 10), (10000, 10))\n"
     ]
    }
   ],
   "source": [
    "dimX = X_train.shape[1]\n",
    "dimY = Y_train.shape[1]\n",
    "nTrain = X_train.shape[0]\n",
    "nTest = X_test.shape[0]\n",
    "print (\"Shape of (X_train, X_test, Y_train, Y_test)\")\n",
    "print (X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어떤 하나의 샘플에 대해 인덱스로 접근해서 영상 및 크기 출력 해봄  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myIdx = 36436   # any number\n",
    "img   = np.reshape(X_train[myIdx, :], (28, 28)) # 28 * 28 = 784\n",
    "\n",
    "plt.matshow(img, cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784) (55000, 10) 784 10\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape, Y_train.shape, dimX, dimY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습을 위한 최소한의 구조를 만들어 본다.\n",
    "#### (1) 먼저 학습 샘플을 저장할 placeholder를 만든다\n",
    "#### (2) 학습 변수를 저장할 W, b를 선언한다\n",
    "#### (3) get_shape()으로 크기를 출력해 본다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 784) (?, 10) (784, 10) (10,)\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32,[None,784],name='input')\n",
    "Y = tf.placeholder(tf.float32,[None,10],name='label')\n",
    "W = tf.Variable(tf.zeros([784,10]), name='weight')\n",
    "b = tf.Variable(tf.zeros([10]), name='bias')\n",
    "print X.get_shape(), Y.get_shape(), W.get_shape(), b.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 변수들을 곱해주는 graph의 노드를 생성한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 10)\n"
     ]
    }
   ],
   "source": [
    "ss = tf.matmul(X,W)\n",
    "y_pred = tf.nn.softmax(tf.add( tf.matmul(X,W) , b))\n",
    "print y_pred.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 샘플은 작은 규모의 batch로 나누어져 입력하기 때문에 loss계산에 평균을 적용한다\n",
    "## 먼저 xentropy loss함수를 구성한다\n",
    "### 각 샘플의 label과 log(predict) 두 값을 곱한 결과를 sum한 후, 모든 샘플에 대해 평균한다.\n",
    "### reduction_indices=1 is along label axis. Usually in matrix, 0(row), 1(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(y_pred), reduction_indices=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss에 대한 gradient를 정의한다. 최적화 인자값을 설정한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "training_epoch = 20\n",
    "batch_size = 100\n",
    "display_epoch = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.argmax와 tf.equal함수를 같이 사용하면 출력값 평가를 간단하게 할 수 있다.\n",
    "### label Y(one-hot)의 최대값과 prediction y_pred의 최대값의 위치가 같은지 체크한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_pred = tf.equal(tf.argmax(Y,1),tf.argmax(y_pred,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트된 모든 샘플들에 대해 correct_pred 값을 평균하여 accuracy값을 계산한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,'float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연산을 위해 Session을 시작한다. 먼저 모든 변수를 초기화하고, 20번의 epoch를 실행한다\n",
    "### batch 샘플의 수를 100개로 했고 이에 따라 전체 batch 덩어리의 수가 결정된다(nBatch)\n",
    "### 사용할 batch 샘플의 index는 매 epoch마다 랜덤하게 결정된다.\n",
    "### 선택된 batch 샘플들은 feed_dict를 통해 넘겨준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch]  1\n",
      "loss = 0.770459\n",
      "accu_train =  0.842582\n",
      "accu_test =  0.8538\n",
      "[epoch]  3\n",
      "loss = 0.592305\n",
      "accu_train =  0.862964\n",
      "accu_test =  0.8729\n",
      "[epoch]  5\n",
      "loss = 0.520089\n",
      "accu_train =  0.872345\n",
      "accu_test =  0.8826\n",
      "[epoch]  7\n",
      "loss = 0.479236\n",
      "accu_train =  0.878982\n",
      "accu_test =  0.8871\n",
      "[epoch]  9\n",
      "loss = 0.452251\n",
      "accu_train =  0.883218\n",
      "accu_test =  0.8912\n",
      "[epoch]  11\n",
      "loss = 0.432806\n",
      "accu_train =  0.886873\n",
      "accu_test =  0.8952\n",
      "[epoch]  13\n",
      "loss = 0.417847\n",
      "accu_train =  0.889618\n",
      "accu_test =  0.8981\n",
      "[epoch]  15\n",
      "loss = 0.405906\n",
      "accu_train =  0.891855\n",
      "accu_test =  0.8991\n",
      "[epoch]  17\n",
      "loss = 0.396161\n",
      "accu_train =  0.894164\n",
      "accu_test =  0.9009\n",
      "[epoch]  19\n",
      "loss = 0.387903\n",
      "accu_train =  0.895473\n",
      "accu_test =  0.903\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    for epoch in range(training_epoch):\n",
    "        nBatch = int(X_train.shape[0]/batch_size)\n",
    "        batch_index = np.random.permutation(X_train.shape[0])           #55,000/100=550(batch 덩어리 수)\n",
    "        for ii in range(nBatch):                                        #1 epoch는 모든 sample data를 거쳐야 끝남\n",
    "            x = X_train[batch_index[ii*batch_size:(ii+1)*batch_size],:]\n",
    "            y = Y_train[batch_index[ii*batch_size:(ii+1)*batch_size],:]\n",
    "            sess.run(optimizer, feed_dict = {X:x, Y:y})\n",
    "            \n",
    "        if (epoch+1)%display_epoch == 0:                                #2 epoch마다 1회씩 결과를 출력\n",
    "            print '[epoch] ',epoch\n",
    "            print 'loss =', loss.eval({X:X_train, Y:Y_train})           #loss.eval(입력): '입력' 값에 대해 loss 계산\n",
    "            print 'accu_train = ', accuracy.eval({X:X_train, Y: Y_train})\n",
    "            print 'accu_test = ', accuracy.eval({X:X_test, Y: Y_test})\n",
    "        \n",
    "        #print sess.run(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
